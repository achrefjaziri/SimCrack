"""
This Dataloder loads an input image and its corresponding segmentation mask.
The images are loaded from  an input directory which contains two directories /inputs and /gsround_truths.
This dataloader expects that the all images in the input folder have a ground truth map with the same naming convention.
"""
import numpy as np
from PIL import Image
import glob
import torch
from torchvision import transforms
from torch.utils.data.dataset import Dataset
import torchvision.transforms.functional as F
import os
import matplotlib.pyplot as plt
from PIL import ImageFile
from lib.arg_parser.general_args import parse_args
from skimage import exposure


class MultiSetDataloader(Dataset):
    # Class for loading data generated by the simulator for training or testing.
    def __init__(self, configs, mode='train'):
        super(MultiSetDataloader, self).__init__()

        # all file names
        self.configs = configs
        imgs_path = os.path.join(self.configs.data_input_dir, 'crack_segmentation_dataset', mode, "images", '*')
        self.image_arr = glob.glob(imgs_path)
        self.img_size = configs.input_size
        self.mode = mode

        # Calculate len
        self.data_len = len(self.image_arr)
        print(f'data length for {mode}', self.data_len)

        if self.configs.resize_crop_input:
            # transformations of the image tensor
            self.scale_size = self.configs.resize_size
            self.img_transforms = transforms.Compose([transforms.Resize([self.scale_size, self.scale_size]),
                                                      # transforms.CenterCrop(self.in_size),transforms.ToTensor()
                                                      ])
            # transformations of the normals tensor
            self.normal_transforms = transforms.Compose([transforms.Resize([self.scale_size, self.scale_size]),
                                                         #transforms.CenterCrop(self.out_size),transforms.ToTensor()
                                                         ])
            # transformations of the segmentation tensor
            self.seg_transforms = transforms.Compose([transforms.Resize([self.scale_size, self.scale_size]),
                                                      #transforms.CenterCrop(self.out_size)
                                                      ])
        else:

            self.img_transforms = transforms.Compose([transforms.Resize([self.img_size, self.img_size]),
                                                      ])

            # transformations for the mask tensor
            self.seg_transforms = transforms.Compose([transforms.Resize([self.img_size, self.img_size]),
                                                       ])

            # transformations of the normals tensor
            self.normal_transforms = transforms.Compose([transforms.Resize([self.img_size, self.img_size]),
                                                         # transforms.CenterCrop(self.out_size),
                                                         transforms.ToTensor()])

    def transform_crop_downsize_combo(self, image, segmask, depth=None, normal=None):
        '''
        Transforms inputs and ground truths by first downsizing to self.scale_size and then randomly cropping to self.input_size
        :param image:
        :param segmask:
        :param depth:
        :param normal:
        :return:
        '''

        ImageFile.LOAD_TRUNCATED_IMAGES = True
        if self.configs.arch_name != 'pmiunet':
            image = self.img_transforms(image)
        mask = self.seg_transforms(segmask)
        if self.configs.arch_name == 'munet':
            normal = self.normal_transforms(normal)

        """Image Cropping"""
        mask_cropped = np.zeros((self.img_size, self.img_size, 4))
        count = 0
        while (np.count_nonzero(mask_cropped[:, :]) < 200) and (
                count < 50):  # makes sure that the training image contains a crack in most cases
            count = count + 1
            i, j, h, w = transforms.RandomCrop.get_params(
                mask, output_size=(self.img_size, self.img_size))

            if self.configs.arch_name!='pmiunet':
                image_cropped = transforms.functional.crop(image, i, j, h, w)
                image_cropped = np.asarray(image_cropped)
            else:
                image_cropped = image[i:i+h,j:j+w,:]

            mask_cropped = transforms.functional.crop(mask, i, j, h, w)
            mask_cropped = np.asarray(mask_cropped)
        # print(image_cropped.shape)

        # delete alpha channel
        input_image = image_cropped[:, :, :3]

        if self.configs.input_ch == 1 and self.configs.arch_name !='pmiunet':
            # greyscale input
            input_image = np.dot(input_image[..., :3], [0.299, 0.587, 0.114])
            input_image = np.expand_dims(input_image, axis=0)
        elif self.configs.input_ch == 1 and self.configs.arch_name =='pmiunet':
            img = input_image[:, :, 0]
            if args.histequalize_pmi:
                img = (img - np.min(img)) / (np.max(img) - np.min(img))
                img = exposure.equalize_adapthist(img, clip_limit=0.03)
            input_image = np.expand_dims(img, axis=0)  # use only one PMI scale
        else:
            input_image = input_image.transpose(2,0,1)  #Permute axis to obtain image with shape (c,h,w)

        mask_torch = torch.from_numpy(np.array(mask_cropped, dtype=int))
        # assing integer values to the pixels
        mask_torch = np.where(mask_torch > 0, 1, mask_torch)
        # delete separate color channel
        #mask_torch = mask_torch[:, :, 0]
        input_torch = torch.tensor(input_image).float()

        return input_torch, mask_torch

    def transform(self, image, mask,pmi_gt=None):
        ImageFile.LOAD_TRUNCATED_IMAGES = True
        """Image Cropping"""
        if self.configs.arch_name != 'pmiunet':
            image = self.img_transforms(image)
        mask = self.seg_transforms(mask)
        # delete alpha channel
        image = np.asarray(image)#.transpose(1,2,0)
        image = image[:, :, :3]

        if self.configs.input_ch == 1:
            # greyscale input
            #image = np.asarray(image).transpose(1,2,0)
            image = np.dot(image[...,:3], [0.299, 0.587, 0.114])
            image = np.expand_dims(image,axis=0)
        elif self.configs.input_ch == 1 and self.configs.arch_name =='pmiunet':
            img_adapteq = exposure.equalize_adapthist(image[:,:,0], clip_limit=0.03)
            image = np.expand_dims(img_adapteq, axis=0) #use only one PMI scale
        else:
            image = image.transpose(2,0,1)  #Permute axis to obtain image with shape (c,h,w)

        mask = torch.from_numpy(np.array(mask, dtype=int))
        # assing integer values to the pixels
        mask = np.where(mask > 0, 1, mask)
        # delete separate color channel
        image = torch.tensor(image).float()

        if self.configs.arch_name=='pmiunet':
            #out = F.interpolate(input_torch, size=(self.input_size, self.input_size), mode='bicubic', align_corners=False)
            image = F.resize(image,size=self.img_size)

        if not pmi_gt is None:
            pmi_gt = pmi_gt[:, :, 0]
            if self.configs.histequalize_pmi:
                pmi_gt = (pmi_gt - np.min(pmi_gt)) / (np.max(pmi_gt) - np.min(pmi_gt))
                pmi_gt = exposure.equalize_adapthist(pmi_gt, clip_limit=0.03)
            pmi_gt = np.expand_dims(pmi_gt, axis=0)  # use only one PMI scale
            pmi_gt = torch.tensor(pmi_gt).float()
            pmi_gt = F.resize(pmi_gt,size=self.img_size)
        return image, mask,pmi_gt

    def __getitem__(self, index):
        img_path = self.image_arr[index]
        mask_path = img_path.replace("images", "masks")  # This probably needs to be changed depending on our structure
        pmi_gt = None
        if self.configs.arch_name == 'pmiunet':
            pmi_path = os.path.join(self.configs.pmi_dir,self.configs.dataset,f'{self.configs.neighbour_size}_{self.configs.phi_value}'
                                    ,self.mode,os.path.basename(mask_path)+'.npy')
            image = np.load(pmi_path)
        elif self.configs.arch_name == 'munet_pmi' or self.configs.arch_name == 'cons_unet':  # we use the pmi as ground truth to train multi path unet
            pmi_path = os.path.join(self.configs.pmi_dir, self.configs.dataset,
                                    f'{self.configs.neighbour_size}_{self.configs.phi_value}'
                                    , self.mode, os.path.basename(mask_path) + '.npy')
            pmi_gt = np.load(pmi_path)
            image = Image.open(img_path)
        else:
            image = Image.open(img_path)

        mask = Image.open(mask_path)
        if self.configs.resize_crop_input:
            img, gt = self.transform_crop_downsize_combo(image, mask)
        else:
            img, gt, pmi_gt = self.transform(image, mask,pmi_gt=pmi_gt)

        if pmi_gt is None:
            data = {'input': img,
                    'gt': gt,
                    'path': mask_path}
        else:
            data = {'input': img,
                    'gt': gt,
                    'pmi_map': pmi_gt,
                    'path': mask_path}
        return data

    def __len__(self):
        return len(self.image_arr)


if __name__ == "__main__":
    args = parse_args()
    Dataset_test = MultiSetDataloader(args,mode='test')

    test_load = \
        torch.utils.data.DataLoader(dataset=Dataset_test,
                                    num_workers=16, batch_size=16, shuffle=False)
    for batch, data in enumerate(test_load):
        input_img = data['input'][0].detach().cpu().numpy()
        target_img = data['gt'][0].detach().cpu().numpy()

        print('image', input_img.shape, target_img.shape)

        plt.figure()
        plt.imshow(input_img[0, :, :])
        plt.show()
        plt.figure()
        plt.imshow(target_img)
        plt.show()

        print('done!!')