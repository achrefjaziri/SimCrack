"""
This Dataloder loads an input image and its corresponding segmentation mask.
The images are loaded from  an input directory which contains two directories /inputs and /gsround_truths.
This dataloader expects that the all images in the input folder have a ground truth map with the same naming convention.
"""
import numpy as np
from PIL import Image
import glob
import torch
from torchvision import transforms
from torch.utils.data.dataset import Dataset
import torchvision.transforms.functional as F
import OpenEXR
import array
import Imath
import cv2
import os
import matplotlib.pyplot as plt
from PIL import ImageFile
from lib.arg_parser.general_args import parse_args
from skimage import exposure



class SimDataloader(Dataset):
    # Class for loading data generated by the simulator for training or testing.
    def __init__(self, configs, mode='train'):
        super(SimDataloader, self).__init__()

        # all file names
        self.configs = configs
        imgs_path = os.path.join(self.configs.data_input_dir,'sim_crack' ,mode, "images", '*')
        self.image_arr = glob.glob(imgs_path)
        self.img_size = configs.input_size
        self.mode = mode

        # Calculate len
        self.data_len = len(self.image_arr)
        print(f'data length for {mode}',self.data_len)

        if self.configs.resize_input:
            # transformations of the image tensor
            self.scale_size = self.configs.resize_size
            self.img_transforms = transforms.Compose([transforms.Resize([self.scale_size, self.scale_size]),
                                                      # transforms.CenterCrop(self.in_size),transforms.ToTensor()
                                                      ])
            # transformations of the normals tensor
            self.normal_transforms = transforms.Compose([transforms.Resize([self.scale_size, self.scale_size]),
                                                         # transforms.CenterCrop(self.out_size),transforms.ToTensor()
                                                         ])
            # transformations of the segmentation tensor
            self.seg_transforms = transforms.Compose([transforms.Resize([self.scale_size, self.scale_size]),
                                                      # transforms.CenterCrop(self.out_size)
                                                      ])

        else:

            self.img_transforms = transforms.Compose([transforms.Resize([self.img_size, self.img_size]),
                                                      ])

            # transformations for the mask tensor
            self.seg_transforms = transforms.Compose([transforms.Resize([self.img_size, self.img_size]),
                                                       ])

            # transformations of the normals tensor
            self.normal_transforms = transforms.Compose([transforms.Resize([self.img_size, self.img_size]),
                                                         # transforms.CenterCrop(self.out_size),
                                                         transforms.ToTensor()])

    def exr2depth(self, depth_files):
        """Transformations of the mask (depth) files"""

        depth_size = 2048
        # Open the input file
        file = OpenEXR.InputFile(depth_files)

        # Compute the size
        dw = file.header()['dataWindow']
        sz = (dw.max.x - dw.min.x + 1, dw.max.y - dw.min.y + 1)

        # Read the three color channels as 32-bit floats
        FLOAT = Imath.PixelType(Imath.PixelType.FLOAT)

        (R, G, B) = [array.array('f', file.channel(Chan, FLOAT)).tolist() for Chan in ("R", "G", "B")]

        # R = np.array(R)
        # G = np.array(G)

        # Size (h, w, 3)
        depth_img = np.zeros((depth_size, depth_size, 3), np.float32)
        # normalize
        depth = np.array(B)
        # depth=(depth - np.min(depth)) / (np.max(depth) - np.min(depth))
        # depth *= 255.0 / depth.max()
        # print(depth)

        depth_img[:, :, 0] = np.array(depth).reshape(depth_img.shape[0], -1)
        depth_img[:, :, 1] = np.array(depth).reshape(depth_img.shape[0], -1)
        depth_img[:, :, 2] = np.array(depth).reshape(depth_img.shape[0], -1)
        # print("min depth: ", np.min(depth))
        # print("max depth: ", np.max(depth))
        # img[:,:,1] = np.array(G).reshape(img.shape[0],-1)

        # Resize the depth to the osize
        depth_img = cv2.resize(depth_img, dsize=(self.img_size, self.img_size), interpolation=cv2.INTER_CUBIC)
        # Centercrop the resized depth
        # bounding = (self.out_size , self.out_size, 3)
        # start = tuple(map(lambda a, da: a // 2 - da // 2 , depth_img.shape, bounding))
        # end = tuple(map(operator.add, start, bounding))
        # slices = tuple(map(slice, start, end))
        # depth_img = depth_img[slices]

        depth_img = np.transpose(depth_img)

        depth_img = torch.from_numpy(depth_img)

        return depth_img

    def transform_crop_downsize_combo(self, image, segmask, depth=None, normal=None):
        '''
        Transforms inputs and ground truths by first downsizing to self.scale_size and then randomly cropping to self.input_size
        :param image:
        :param segmask:
        :param depth:
        :param normal:
        :return:
        '''

        ImageFile.LOAD_TRUNCATED_IMAGES = True
        if self.configs.arch_name != 'pmiunet':
            image = self.img_transforms(image)
        mask = self.seg_transforms(segmask)
        if self.configs.arch_name == 'munet':
            normal = self.normal_transforms(normal)

        """Image Cropping"""
        mask_cropped = np.zeros((self.img_size, self.img_size, 4))
        count = 0
        while (np.count_nonzero(mask_cropped[:, :, 0]) < 200) and (
                count < 50):  # makes sure that the training image contains a crack in most cases
            count = count + 1
            i, j, h, w = transforms.RandomCrop.get_params(
                mask, output_size=(self.img_size, self.img_size))

            if self.configs.arch_name!='pmiunet':
                image_cropped = transforms.functional.crop(image, i, j, h, w)
                image_cropped = np.asarray(image_cropped)
            else:
                image_cropped = image[i:i+h,j:j+w,:]

            if self.configs.arch_name == 'munet':
                nrm_cropped = transforms.functional.crop(normal, i, j, h, w)
            # nrm_cropped = np.asarray(nrm_cropped)

            mask_cropped = transforms.functional.crop(mask, i, j, h, w)
            mask_cropped = np.asarray(mask_cropped)
        # print(image_cropped.shape)

        # delete alpha channel
        input_image = image_cropped[:, :, :3]

        if self.configs.input_ch == 1 and self.configs.arch_name !='pmiunet':
            # greyscale input
            input_image = np.dot(input_image[..., :3], [0.299, 0.587, 0.114])
            input_image = np.expand_dims(input_image, axis=0)
        elif self.configs.input_ch == 1 and self.configs.arch_name =='pmiunet':
            img  = input_image[:, :, 0]
            img = (img - np.min(img)) / (np.max(img) - np.min(img))
            img_adapteq = exposure.equalize_adapthist(img, clip_limit=0.03)
            input_image = np.expand_dims(img_adapteq, axis=0)  # use only one PMI scale
        else:
            input_image = input_image.transpose(2,0,1)  #Permute axis to obtain image with shape (c,h,w)

        mask_torch = torch.from_numpy(np.array(mask_cropped, dtype=int))
        # assing integer values to the pixels
        mask_torch = np.where(mask_torch > 0, 1, mask_torch)
        # delete separate color channel
        mask_torch = mask_torch[:, :, 0]
        input_torch = torch.tensor(input_image).float()

        if self.configs.arch_name == 'munet':
            self.transfo_to_tensor = transforms.Compose([
                transforms.ToTensor()])
            nrm_image = self.transfo_to_tensor(nrm_cropped)
            nrm_image = nrm_image[:3, :, :]

            # i, j, h, w

            # just use one channel since all channels have equal values
            depth = depth[:1, i:h + i, j:w + j]

            return input_torch, mask_torch, depth, nrm_image
        return input_torch, mask_torch

    def transform(self, image, mask, depth=None, normal=None):
        ImageFile.LOAD_TRUNCATED_IMAGES = True
        """Image Cropping"""
        if self.configs.arch_name != 'pmiunet':
            image = self.img_transforms(image)
        mask = self.seg_transforms(mask)
        # delete alpha channel
        image = np.asarray(image)#.transpose(1,2,0)
        image = image[:, :, :3]

        if self.configs.input_ch == 1:
            # greyscale input
            #image = np.asarray(image).transpose(1,2,0)
            image = np.dot(image[...,:3], [0.299, 0.587, 0.114])
            image = np.expand_dims(image,axis=0)
        elif self.configs.input_ch == 1 and self.configs.arch_name =='pmiunet':
            img_adapteq = exposure.equalize_adapthist(image[:,:,0], clip_limit=0.03)
            image = np.expand_dims(img_adapteq, axis=0) #use only one PMI scale
        else:
            image = image.transpose(2,0,1)  #Permute axis to obtain image with shape (c,h,w)

        mask = torch.from_numpy(np.array(mask, dtype=int))
        # assing integer values to the pixels
        mask = np.where(mask > 0, 1, mask)
        # delete separate color channel
        mask = mask[:, :, 0]
        image = torch.tensor(image).float()

        if self.configs.arch_name=='pmiunet':
            #out = F.interpolate(input_torch, size=(self.input_size, self.input_size), mode='bicubic', align_corners=False)
            image = F.resize(image,size=self.img_size)

        if self.configs.arch_name == 'munet':
            normal = self.normal_transforms(normal)
            # delete alpha channel
            normal = normal[:3, :, :]
            # just use one channel since all channels have equal values
            depth = depth[:1, :, :]
            return image, mask, depth, normal

        return image, mask

    def __getitem__(self, index):
        img_path = self.image_arr[index]
        mask_path = img_path.replace("images", "gts").replace('render',
                                                                'gt')  # This probably needs to be changed depending on our structure
        if self.configs.arch_name == 'pmiunet':
            pmi_path = os.path.join(self.configs.pmi_dir,f'{self.configs.neighbour_size}_{self.configs.phi_value}'
                                    ,self.mode,os.path.basename(mask_path)+'.npy')
            image = np.load(pmi_path)
        else:
            image = Image.open(img_path)
        mask = Image.open(mask_path)
        if self.configs.arch_name == 'munet':
            depth_path = img_path.replace("images", "depths").replace('render',
                                                                      'depth').replace('png',
                                                                                       'exr')  # This probably needs to be changed depending on our structure
            normal_path = img_path.replace("images", "normals").replace('render',
                                                                        'normal')  # This probably needs to be changed depending on our structure
            try:
                depth = self.exr2depth(depth_path)
            except OSError:
                print(depth_path, 'is corrupted')
                depth_img = np.zeros((3, self.img_size, self.img_size), np.float32)
                depth = torch.from_numpy(depth_img)
            normal = Image.open(normal_path)
            if self.configs.resize_input:
                img, segmask, depth, normal = self.transform_crop_downsize_combo(image, mask, depth, normal)
            else:
                img, segmask, depth, normal = self.transform(image, mask, depth, normal)
            data = {'input': img, 'gt': segmask, 'depth': depth,
                    'normal': normal, 'path': mask_path}
        else:
            if self.configs.resize_input:
                img, gt = self.transform_crop_downsize_combo(image, mask)
            else:
                img, gt = self.transform(image, mask)
            data = {'input': img,
                    'gt': gt,
                    'path': mask_path}
        return data

    def __len__(self):
        return len(self.image_arr)




if __name__ == "__main__":
    args = parse_args()
    Dataset_test = SimDataloader(args,mode='val')

    test_load = \
        torch.utils.data.DataLoader(dataset=Dataset_test,
                                    num_workers=16, batch_size=16, shuffle=False)
    for batch, data in enumerate(test_load):
        input_img = data['input'][0].detach().cpu().numpy()
        target_img = data['gt'][0].detach().cpu().numpy()

        print('image', input_img.shape, target_img.shape)

        plt.figure()
        plt.imshow(input_img[0, :, :])
        plt.show()
        plt.figure()
        plt.imshow(target_img)
        plt.show()

    print('done!!')

        #plt.Figure()
        #plt.imshow(input_img,cmap='gray')
        #plt.savefig(f"./sim_examples/input{batch}.png")

        #plt.Figure()
        #plt.imshow(target_img,cmap='gray')
        #plt.savefig(f"./sim_examples/gt{batch}.png")