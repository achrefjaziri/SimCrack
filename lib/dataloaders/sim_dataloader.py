"""
This Dataloder loads an input image and its corresponding segmentation mask.
The images are loaded from  an input directory which contains two directories /inputs and /gsround_truths.
This dataloader expects that the all images in the input folder have a ground truth map with the same naming convention.
"""
import numpy as np
from PIL import Image
import glob
import torch
import torch.nn as nn
from torchvision import transforms
from torch.utils.data.dataset import Dataset
import torchvision.transforms.functional as F
import OpenEXR
import array
import Imath
import cv2
import os
import matplotlib.pyplot as plt
from PIL import ImageFile
from lib.arg_parser.general_args import parse_args
from skimage import exposure
import kornia
class SimDataloader(Dataset):
    # Class for loading data generated by the simulator for training or testing.
    def __init__(self, configs, mode='train',set_size='',filtering=False,subset=False,subset_nb=0,remove_duplicated='/data/resist_data/adaIn_maps/sim_crack/train/*'):
        super(SimDataloader, self).__init__()

        # all file names
        self.configs = configs
        imgs_path = os.path.join(self.configs.data_input_dir,'sim_crack' ,mode, "images", '*')
        self.image_arr = glob.glob(imgs_path)
        if filtering:
            print('Before Filtering',len(self.image_arr))

            if not self.image_arr is None:
                already_done = glob.glob(remove_duplicated)
                print(len(already_done))
                already_done = [os.path.join(os.path.dirname(self.image_arr[0]),os.path.basename(pp).replace('gt','render').replace('.npy','')) for pp in already_done]
                self.image_arr = list(set(self.image_arr) ^ set(already_done))
                print('done filtering')
            print('After Filtering',len(self.image_arr))

            if subset:
                self.image_arr.sort()
                self.image_arr = np.array_split(np.array(self.image_arr),10)[subset_nb].tolist()
            print('After Subset', len(self.image_arr))

        self.img_size = configs.input_size
        self.mode = mode

        # Calculate len
        self.data_len = len(self.image_arr)
        if set_size!='':
            self.image_arr.sort()
            self.image_arr = self.image_arr[:int(set_size)]
        print(f'data length for {mode}',self.data_len)

        if self.configs.resize_crop_input:
            # transformations of the image tensor
            self.scale_size = self.configs.resize_size
            self.img_transforms = transforms.Compose([transforms.Resize([self.scale_size, self.scale_size]),
                                                      # transforms.CenterCrop(self.in_size),transforms.ToTensor()
                                                      ])
            # transformations of the normals tensor
            self.normal_transforms = transforms.Compose([transforms.Resize([self.scale_size, self.scale_size]),
                                                         # transforms.CenterCrop(self.out_size),transforms.ToTensor()
                                                         ])
            # transformations of the segmentation tensor
            self.seg_transforms = transforms.Compose([transforms.Resize([self.scale_size, self.scale_size]),
                                                      # transforms.CenterCrop(self.out_size)
                                                      ])

        else:

            self.img_transforms = transforms.Compose([transforms.Resize([self.img_size, self.img_size]),
                                                      ])

            # transformations for the mask tensor
            self.seg_transforms = transforms.Compose([transforms.Resize([self.img_size, self.img_size]),
                                                       ])

            # transformations of the normals tensor
            self.normal_transforms = transforms.Compose([transforms.Resize([self.img_size, self.img_size]),
                                                         # transforms.CenterCrop(self.out_size),
                                                         transforms.ToTensor()])

    def exr2depth(self, depth_files):
        """Transformations of the mask (depth) files"""

        depth_size = 2048
        # Open the input file
        file = OpenEXR.InputFile(depth_files)

        # Compute the size
        dw = file.header()['dataWindow']
        sz = (dw.max.x - dw.min.x + 1, dw.max.y - dw.min.y + 1)

        # Read the three color channels as 32-bit floats
        FLOAT = Imath.PixelType(Imath.PixelType.FLOAT)

        (R, G, B) = [array.array('f', file.channel(Chan, FLOAT)).tolist() for Chan in ("R", "G", "B")]

        # R = np.array(R)
        # G = np.array(G)

        # Size (h, w, 3)
        depth_img = np.zeros((depth_size, depth_size, 3), np.float32)
        # normalize
        depth = np.array(B)
        # depth=(depth - np.min(depth)) / (np.max(depth) - np.min(depth))
        # depth *= 255.0 / depth.max()
        # print(depth)

        depth_img[:, :, 0] = np.array(depth).reshape(depth_img.shape[0], -1)
        depth_img[:, :, 1] = np.array(depth).reshape(depth_img.shape[0], -1)
        depth_img[:, :, 2] = np.array(depth).reshape(depth_img.shape[0], -1)
        # print("min depth: ", np.min(depth))
        # print("max depth: ", np.max(depth))
        # img[:,:,1] = np.array(G).reshape(img.shape[0],-1)

        # Resize the depth to the osize
        if self.configs.resize_crop_input:
            depth_img = cv2.resize(depth_img, dsize=(self.scale_size, self.scale_size), interpolation=cv2.INTER_CUBIC)
        else:
            depth_img = cv2.resize(depth_img, dsize=(self.img_size, self.img_size), interpolation=cv2.INTER_CUBIC)
        # Centercrop the resized depth
        # bounding = (self.out_size , self.out_size, 3)
        # start = tuple(map(lambda a, da: a // 2 - da // 2 , depth_img.shape, bounding))
        # end = tuple(map(operator.add, start, bounding))
        # slices = tuple(map(slice, start, end))
        # depth_img = depth_img[slices]

        depth_img = np.transpose(depth_img)

        depth_img = torch.from_numpy(depth_img)

        return depth_img

    def transform_crop_downsize_combo(self, image, segmask, depth=None, normal=None,pmi_gt= None):
        '''
        Transforms inputs and ground truths by first downsizing to self.scale_size and then randomly cropping to self.input_size
        :param image:
        :param segmask:
        :param depth:
        :param normal:
        :param pmi_gt
        :return:
        '''

        ImageFile.LOAD_TRUNCATED_IMAGES = True
        if self.configs.arch_name != 'pmiunet':
            image = self.img_transforms(image)
        mask = self.seg_transforms(segmask)
        if self.configs.arch_name == 'munet':
            normal = self.normal_transforms(normal)

        """Image Cropping"""
        mask_cropped = np.zeros((self.img_size, self.img_size, 4))
        count = 0
        while (np.count_nonzero(mask_cropped[:, :, 0]) < 200) and (
                count < 50):  # makes sure that the training image contains a crack in most cases
            count = count + 1
            i, j, h, w = transforms.RandomCrop.get_params(
                mask, output_size=(self.img_size, self.img_size))

            if self.configs.arch_name!='pmiunet':
                image_cropped = transforms.functional.crop(image, i, j, h, w)
                image_cropped = np.asarray(image_cropped)
                if not pmi_gt is None:
                    pmi_gt_cropped = pmi_gt[i:i + h, j:j + w, :]
            else:
                image_cropped = image[i:i+h,j:j+w,:]

            if self.configs.arch_name == 'munet':
                nrm_cropped = transforms.functional.crop(normal, i, j, h, w)
            # nrm_cropped = np.asarray(nrm_cropped)

            mask_cropped = transforms.functional.crop(mask, i, j, h, w)
            mask_cropped = np.asarray(mask_cropped)

        # delete alpha channel
        input_image = image_cropped[:, :, :3]
        if not pmi_gt is None:
            pmi_gt = pmi_gt_cropped
        if self.configs.input_ch == 1 and self.configs.arch_name !='pmiunet':
            # greyscale input
            input_image = np.dot(input_image[..., :3], [0.299, 0.587, 0.114])
            input_image = np.expand_dims(input_image, axis=0)
            input_torch = torch.tensor(input_image).float()
            if not pmi_gt is None:
                pmi_gt = pmi_gt[:, :, 0]
                if self.configs.histequalize_pmi:
                    pmi_gt = (pmi_gt - np.min(pmi_gt)) / (np.max(pmi_gt) - np.min(pmi_gt))
                    pmi_gt = exposure.equalize_adapthist(pmi_gt, clip_limit=0.03)
                pmi_gt = np.expand_dims(pmi_gt, axis=0)  # use only one PMI scale
                pmi_gt = torch.tensor(pmi_gt).float()
        elif self.configs.input_ch == 1 and self.configs.arch_name =='pmiunet':
            img  = input_image[:, :, 0]
            if self.configs.histequalize_pmi:
                img = (img - np.min(img)) / (np.max(img) - np.min(img))
                img = exposure.equalize_adapthist(img, clip_limit=0.03)
            input_image = np.expand_dims(img, axis=0)  # use only one PMI scale
            input_torch = torch.tensor(input_image).float()

        else:
            input_image = input_image.transpose(2,0,1)  #Permute axis to obtain image with shape (c,h,w)
            input_torch = torch.tensor(input_image).float()

        mask_torch = torch.from_numpy(np.array(mask_cropped, dtype=int))
        # assing integer values to the pixels
        mask_torch = np.where(mask_torch > 0, 1, mask_torch)
        # delete separate color channel
        mask_torch = mask_torch[:, :, 0]


        if self.configs.arch_name == 'munet':
            self.transfo_to_tensor = transforms.Compose([
                transforms.ToTensor()])
            nrm_image = self.transfo_to_tensor(nrm_cropped)
            nrm_image = nrm_image[:3, :, :]

            # i, j, h, w

            # just use one channel since all channels have equal values
            depth = depth[:1, i:h + i, j:w + j]

            return input_torch, mask_torch, depth, nrm_image
        return input_torch, mask_torch,pmi_gt

    def transform(self, image, mask, depth=None, normal=None,pmi_gt=None):
        ImageFile.LOAD_TRUNCATED_IMAGES = True
        """Image Cropping"""
        if self.configs.arch_name != 'pmiunet':
            image = self.img_transforms(image)
        mask = self.seg_transforms(mask)
        # delete alpha channel
        image = np.asarray(image)#.transpose(1,2,0)
        image = image[:, :, :3]

        #TODO change this to add pmi_gt
        if self.configs.input_ch == 1:
            # greyscale input
            #image = np.asarray(image).transpose(1,2,0)
            image = np.dot(image[...,:3], [0.299, 0.587, 0.114])
            image = np.expand_dims(image,axis=0)
        elif self.configs.input_ch == 1 and self.configs.arch_name =='pmiunet':
            img_adapteq = exposure.equalize_adapthist(image[:,:,0], clip_limit=0.03)
            image = np.expand_dims(img_adapteq, axis=0) #use only one PMI scale
        else:
            image = image.transpose(2,0,1)  #Permute axis to obtain image with shape (c,h,w)

        mask = torch.from_numpy(np.array(mask, dtype=int))
        # assing integer values to the pixels
        mask = np.where(mask > 0, 1, mask)
        # delete separate color channel
        mask = mask[:, :, 0]
        image = torch.tensor(image).float()

        if self.configs.arch_name=='pmiunet':
            #out = F.interpolate(input_torch, size=(self.input_size, self.input_size), mode='bicubic', align_corners=False)
            image = F.resize(image,size=self.img_size)
        if not pmi_gt is None:
            pmi_gt = pmi_gt[:, :, 0]
            if self.configs.histequalize_pmi:
                pmi_gt = (pmi_gt - np.min(pmi_gt)) / (np.max(pmi_gt) - np.min(pmi_gt))
                pmi_gt = exposure.equalize_adapthist(pmi_gt, clip_limit=0.03)
            pmi_gt = np.expand_dims(pmi_gt, axis=0)  # use only one PMI scale
            pmi_gt = torch.tensor(pmi_gt).float()
            pmi_gt = F.resize(pmi_gt,size=self.img_size)

        if self.configs.arch_name == 'munet':
            normal = self.normal_transforms(normal)
            # delete alpha channel
            normal = normal[:3, :, :]
            # just use one channel since all channels have equal values
            depth = depth[:1, :, :]
            return image, mask, depth, normal

        return image, mask, pmi_gt

    def __getitem__(self, index):
        img_path = self.image_arr[index]
        mask_path = img_path.replace("images", "gts").replace('render',
                                                                'gt')  # This probably needs to be changed depending on our structure
        pmi_gt = None
        if self.configs.arch_name == 'pmiunet':
            pmi_path = os.path.join(self.configs.pmi_dir,f'{self.configs.neighbour_size}_{self.configs.phi_value}'
                                    ,self.mode,os.path.basename(mask_path)+'.npy')
            image = np.load(pmi_path)
        elif self.configs.arch_name == 'munet_pmi' or self.configs.arch_name == 'cons_unet': #we use the pmi as ground truth to train multi path unet
            pmi_path = os.path.join(self.configs.pmi_dir, f'{self.configs.neighbour_size}_{self.configs.phi_value}'
                                    , self.mode, os.path.basename(mask_path) + '.npy')
            pmi_gt = np.load(pmi_path)
            image = Image.open(img_path)
        else:
            image = Image.open(img_path)
        mask = Image.open(mask_path)
        if self.configs.arch_name == 'munet':
            depth_path = img_path.replace("images", "depths").replace('render',
                                                                      'depth').replace('png',
                                                                                       'exr')  # This probably needs to be changed depending on our structure
            normal_path = img_path.replace("images", "normals").replace('render',
                                                                        'normal')  # This probably needs to be changed depending on our structure
            try:
                depth = self.exr2depth(depth_path)
            except OSError:
                print(depth_path, 'is corrupted')
                depth_img = np.zeros((3, self.img_size, self.img_size), np.float32)
                depth = torch.from_numpy(depth_img)
            normal = Image.open(normal_path)
            if self.configs.resize_crop_input:
                img, segmask, depth, normal = self.transform_crop_downsize_combo(image, mask, depth, normal)
            else:
                img, segmask, depth, normal = self.transform(image, mask, depth, normal)
            data = {'input': img, 'gt': segmask, 'depth': depth,
                    'normal': normal, 'path': mask_path}
        else:
            if self.configs.resize_crop_input:
                img, gt,pmi_gt = self.transform_crop_downsize_combo(image, mask,pmi_gt=pmi_gt)
            else:
                img, gt,pmi_gt = self.transform(image, mask,pmi_gt=pmi_gt)

            if pmi_gt is None:
                data = {'input': img,
                        'gt': gt,
                        'path': mask_path}
            else:
                data = {'input': img,
                    'gt': gt,
                    'pmi_map':pmi_gt,
                    'path': mask_path}
        return data

    def __len__(self):
        return len(self.image_arr)




if __name__ == "__main__":
    args = parse_args()
    Dataset_test = SimDataloader(args,mode='train')

    test_load = \
        torch.utils.data.DataLoader(dataset=Dataset_test,
                                    num_workers=16, batch_size=16, shuffle=False)
    for batch, data in enumerate(test_load):
        input_img = data['input'].detach().cpu().numpy()
        target_img = data['pmi_map'][0].detach().cpu().numpy()
        gt = data['gt'][0].detach().cpu().numpy()

        print('image', input_img.shape, target_img.shape,gt.shape)

        plt.figure()
        plt.imshow(input_img[:, :, :].transpose(1,2,0))
        plt.show()
        #plt.figure()
        #plt.imshow(target_img)
        #plt.show()


        #plt.Figure()
        #plt.imshow(input_img,cmap='gray')
        #plt.savefig(f"./sim_examples/input{batch}.png")

        #plt.Figure()
        #plt.imshow(target_img,cmap='gray')
        #plt.savefig(f"./sim_examples/gt{batch}.png")